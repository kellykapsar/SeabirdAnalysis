################################################################################
# TITLE: Speed Hex Data Processing
# PURPOSE: This script takes raw AIS data from exactEarth and processes it into 
  # summary statistics aggregated into hexagonal polygons (generated by 
  # H1_HexGridCreation.R script).Summary statistics include number of ships,
  # number of operating days, average speed, and sd of speed on a monthly scale for 
  # cargo, fishing, tanker, other, long-fast, and all vessels combined.
# AUTHOR: Ben Sullender & Kelly Kapsar
# CREATED: 2021
# LAST UPDATED ON: 2022-06-09
# 
# NOTE: CODE DESIGNED TO RUN ON HPCC. 
################################################################################

# Start timer
start <- proc.time()

# Load libraries 
library(maptools)
library(rgdal)
library(dplyr)
library(tidyr)
library(tibble)
library(sf)
library(foreach)
library(doParallel)

####################################################################
##################### AIS PROCESSING FUNCTION ######################
####################################################################

# INPUTS: A list of lists containing all daily csv file names for one year of AIS data.  
## Inner list = file paths/names for daily AIS csvs
## Outer list = a list of months 

# OUTPUTS: 
## Monthly vector files (hex grid) containing ship traffic summary statistics 
## by ship type and ship length. 

FWS.AIS.SpeedHex <- function(csvList, hexgrid){
  # start timer 
  starttime <- proc.time()
  
  ##################### Initial data import ######################
  # Start segment timer 
  start <- proc.time()
  
  # this will come in handy later. chars 28 to 34 = "yyyy-mm"
  MoName <- substr(csvList[[1]][1],77, 83) # MY COMPUTER
  # MoName <- substr(csvList[[1]][1],45, 51) # HPCC 
  yr <- substr(MoName, 1, 4) 
  mnth <- substr(MoName, 6, 7)
  print(paste0("Processing",yr, mnth))
  
  # set up dfs for metadata and runtimes
  metadata <- data.frame(yr = yr, mnth=mnth)
  runtimes <- data.frame(yr = yr, mnth=mnth)
  
  # read in csv (specifying only columns that we want based on position in dataframe)
  temp <-  lapply(csvList, read.csv, header=TRUE, na.strings=c("","NA"),
                  colClasses = c(rep("character", 2), "NULL", "character", "NULL", "NULL", 
                                 "character", rep("NULL", 6), "character", "NULL", rep("character",8),
                                 "NULL", "character", "NULL", "character", "NULL", rep("character", 2), rep("NULL", 109)
                  ))
  
  
  AIScsv <- do.call(rbind , temp)
  
  metadata$orig_MMSIs <- length(unique(AIScsv$MMSI))
  metadata$orig_pts <- length(AIScsv$MMSI)
  
  runtimes$importtime <- (proc.time() - start)[[3]]/60
  
  # print(paste("Imported: ",yr, mnth))
  ##################### Data cleaning ######################
  start <- proc.time()
  
  # Convert character columns to numeric as needed
  numcols <- c(1:2, 6:12, 14:17)
  AIScsv[,numcols] <- lapply(AIScsv[,numcols], as.numeric)
  
  # print(paste("Numeric cols: ",yr, mnth))
  # Create df using only position messages (excluding type 27 which has increased location error)
  # For more info on message types see: https://www.marinfo.gc.ca/e-nav/docs/list-of-ais-messages-en.php
  AIScsvDF5 <- AIScsv %>%
    dplyr::select(MMSI,Latitude,Longitude,Time,Message_ID,SOG) %>%
    subset(!(Message_ID %in% c(5,24, 27)))
  
  metadata$messid_mmsis <- length(unique(AIScsvDF5$MMSI))
  metadata$messid_pts <- length(AIScsvDF5$MMSI)
  
  # print(paste("Position messages: ",yr, mnth))
  # Remove invalid lat/long values
  AIScsvDF4 <- AIScsvDF5 %>%
    filter(!is.na(Latitude)) %>%
    filter(!is.na(Longitude)) 
  
  metadata$invallatlon__mmsis <- length(unique(AIScsvDF4$MMSI))
  metadata$invallatlon__pts <- length(AIScsvDF4$MMSI)
  
  # print(paste("Inval lat/lon: ",yr, mnth))
  # Remove invalid MMSIs
  AIScsvDF3 <- AIScsvDF4 %>%
    dplyr::filter(nchar(trunc(abs(MMSI))) == 9)
  
  metadata$invalmmsi__mmsis <- length(unique(AIScsvDF3$MMSI))
  metadata$invalmmsi__pts <- length(AIScsvDF3$MMSI)
  
  # print(paste("Inval MMSI: ",yr, mnth))
  # Remove stationary aids to navigation
  AIScsvDF2 <- AIScsvDF3 %>%
    dplyr::filter(MMSI < 990000000) 
  
  metadata$aton_mmsis <- length(unique(AIScsvDF2$MMSI))
  metadata$aton_pts <- length(AIScsvDF2$MMSI)
  
  # Create AIS_ID field
  AIScsvDF <- AIScsvDF2 %>% add_column(AIS_ID = paste0(AIScsvDF2$MMSI,"-",substr(AIScsvDF2$Time,1,8)))
  
  # Identify and remove frost flowers 
  # (i.e. multiple messages from the same exact location sporadically transmitted throughout the day)
  ff <- AIScsvDF %>% 
    group_by(AIS_ID, Longitude, Latitude) %>% 
    summarize(n=n()) %>% filter(n > 2) %>% 
    mutate(tempid = paste0(AIS_ID, Longitude, Latitude))
  
  AISspeed4 <- AIScsvDF %>% 
    mutate(tempid = paste0(AIS_ID, Longitude, Latitude)) %>% 
    filter(!(tempid %in% ff$tempid))
  
  runtimes$dftime <- (proc.time() - start)[[3]]/60
  
  print(paste("Finished Cleaning ",yr, mnth))
  
  ##################### Speed filtering ######################
  
  # Convert to spatial data, get coords in projected format, and intersect with hex grid 
  AISspeed3 <- AISspeed4 %>%
    st_as_sf(coords=c("Longitude","Latitude"),crs=4326) %>%
    # project into Alaska Albers (or other CRS that doesn't create huge gap in mid-Bering with -180W and 180E)
    st_transform(crs=3338) %>%
    st_join(hexgrid["hexID"]) %>% 
    mutate(Time = as.POSIXct(Time, format="%Y%m%d_%H%M%OS")) %>% 
    arrange(Time) 
  
  dftime <- (proc.time() - start)[[3]]/60
  start <- proc.time()
  
  # Calculate the euclidean speed between points
  # Also remove successive duplicate points (i.e., same time stamp and/or same location)
  AISspeed3[, c("long", "lat")] <- st_coordinates(AISspeed3)
  AISspeed2 <- AISspeed3 %>% 
    group_by(AIS_ID) %>%
    arrange(AIS_ID, Time) %>% 
    mutate(timediff = as.numeric(difftime(Time,lag(Time),units=c("hours"))),
           distdiff = sqrt((lat-lag(lat))^2 + (long-lag(long))^2)/1000) %>% 
    filter(timediff > 0) %>% 
    filter(distdiff > 0) %>% 
    mutate(speed = distdiff/timediff) 
  
  metadata$redund_aisids <- length(unique(AISspeed2$AIS_ID))
  metadata$redund_mmsi <- length(unique(AISspeed2$MMSI))
  metadata$redund_pts <- length(AISspeed2$MMSI)
  
  # Implement speed filter of 100 km/hr (also remove NA speed)
  AISspeed1 <- AISspeed2 %>% dplyr::filter(speed < 100) %>% dplyr::filter(!is.na(speed))
  
  metadata$speed_aisids <- length(unique(AISspeed1$AIS_ID))
  metadata$speed_mmsi <- length(unique(AISspeed1$MMSI))
  metadata$speed_pts <- length(AISspeed1$MMSI)
  
  # Remove points outside of hex grid 
  # system.time(joinTest <- joinTest_step1[!is.na(joinTest_step1$hexID),]) # Slow
  # system.time(na.omit(joinTest_step1[,c("hexID")])) # Slowest
  # system.time(joinTest_step1[-which(is.na(joinTest_step1$hexID)),]) # Slower 
  AISspeed <- subset(AISspeed1, !is.na(AISspeed1$hexID)) # Fastest
  
  metadata$sinhex_aisids <- length(unique(AISspeed$AIS_ID))
  metadata$sinhex_mmsis <- length(unique(AISspeed$MMSI))
  metadata$sinhex_pts <- length(AISspeed$MMSI)
  
  runtimes$nothextime <- (proc.time() - start)[[3]]/60

  # Calculate whether points are occurring during daytime or at night 
  if(daynight==TRUE){
    temp <- st_coordinates(st_transform(AISspeed, 4326))
    print(paste("Spatial ",yr, mnth))
    sunrise <- maptools::sunriset(temp, dateTime=AISspeed$Time, direction="sunrise", POSIXct.out=TRUE)
    sunset <- maptools::sunriset(temp, dateTime=AISspeed$Time, direction="sunset", POSIXct.out=TRUE)
    print(paste("Sunrise",yr, mnth))
    AISspeed$timeofday <- ifelse(AISspeed$Time > sunrise$time & AISspeed$Time < sunset$time, "day","night")
    AISspeed$timeofday <- as.factor(AISspeed$timeofday)
  }
  
  start <- proc.time()
  
  # create lookup table from static messages
  AISlookup1 <- AIScsv %>%
    add_column(DimLength = AIScsv$Dimension_to_Bow+AIScsv$Dimension_to_stern, 
               DimWidth = AIScsv$Dimension_to_port+AIScsv$Dimension_to_starboard) %>%
    dplyr::select(-Navigational_status, 
                  -SOG, 
                  -Longitude, 
                  -Latitude) %>%
    filter(Message_ID %in% c(5,24)) %>%
    filter(nchar(trunc(abs(MMSI))) == 9) 
  
  # Identify "best" static message from a given day 
  # Take all the static messages from a given month and down weight messages with a ship 
  # type of 0 or NA. Take the static messages and keeps the message with the highest weight
  # Code adapted from: https://stackoverflow.com/questions/72650475/take-unique-rows-in-r-but-keep-most-common-value-of-a-column-and-use-hierarchy
  wghts <- data.frame(poss = c(0, NA), nums = c(-10, -10))
  matched <- left_join(AISlookup1, wghts, by = c("Ship_Type"= "poss"))
  matched$nums[is.na(matched$nums)] <- 1
  data.table::setDT(matched)[, freq := .N, by = c("MMSI", "Ship_Type")]
  multiplied <- distinct(matched, MMSI, Ship_Type, .keep_all = TRUE)
  multiplied$mult <- multiplied$nums * multiplied$freq
  check <- multiplied[with(multiplied, order(MMSI, -mult)), ]
  AISlookup <- distinct(check, MMSI, .keep_all = TRUE) %>% dplyr::select(-nums, -freq, -mult)
  
  metadata$InSfNotLookup_mmsis <- length(AISspeed$MMSI[!(AISspeed$MMSI %in% AISlookup$MMSI)])
  metadata$InLookupNotSf_mmsis <- length(AISlookup$MMSI[!(AISlookup$MMSI %in% AISspeed$MMSI)])
  
  
  # Calculate number of ships with 0 values in dimensions and convert to NA
  metadata$nolength <- length(which(is.na(AISlookup$Dimension_to_Bow | AISlookup$Dimension_to_stern)))
  zerolength <- which(AISlookup$Dimension_to_Bow == 0 | AISlookup$Dimension_to_stern == 0)
  metadata$pctzerolength <- round((metadata$nolength + length(zerolength))/length(AISlookup$Dimension_to_Bow)*100,2)
  
  metadata$nowidth <- length(which(is.na(AISlookup$Dimension_to_port | AISlookup$Dimension_to_starboard)))
  zerowidth <- which(AISlookup$Dimension_to_port == 0 | AISlookup$Dimension_to_starboard == 0)
  metadata$pctzerowidth <- round((metadata$nowidth + length(zerowidth))/length(AISlookup$Dimension_to_port)*100,2)
  
  # Remove zero value rows for consideration of respective measurement
  # (i.e. if either bow or stern is zero, then both get NA 
  # and if either port or starboard is zero then both get NA)
  AISlookup$Dimension_to_Bow[zerolength] <- NA
  AISlookup$Dimension_to_stern[zerolength] <- NA
  AISlookup$Dimension_to_port[zerowidth] <- NA
  AISlookup$Dimension_to_starboard[zerowidth] <- NA
  

  # Join lookup table to the lines based on scramble mmsi
  AISjoined <- st_drop_geometry(AISspeed) %>%
    left_join(AISlookup,by="MMSI")
  
  # ID ship type 
  # link to ship type/numbers table: 
  # https://help.marinetraffic.com/hc/en-us/articles/205579997-What-is-the-significance-of-the-AIS-Shiptype-number-
  AISjoined$AIS_Type <- ifelse(substr(AISjoined$Ship_Type,1,1)==7, "Cargo",
                        ifelse(substr(AISjoined$Ship_Type,1,1)==8, "Tanker", 
                        ifelse(substr(AISjoined$Ship_Type,1,2)==30, "Fishing", 
                        ifelse(substr(AISjoined$Ship_Type,1,2)==52, "Tug",
                        ifelse(substr(AISjoined$Ship_Type,1,1)==6, "Passenger",
                        ifelse(substr(AISjoined$Ship_Type,1,2)==36, "Sailing",
                        ifelse(substr(AISjoined$Ship_Type,1,2)==37, "Pleasure", "Other")))))))
  AISjoined$AIS_Type[which(is.na(AISjoined$AIS_Type))] <- "Other"
  
  jointime <- (proc.time() - start)[[3]]/60
  start <- proc.time()
  
  # Number of points by ship type
  nships <- AISjoined %>% group_by(AIS_Type) %>% summarize(n=length(unique(MMSI)))
  metadata$ntank_mmsis <- nships$n[nships$AIS_Type == "Tanker"]
  metadata$ntug_mmsis <- nships$n[nships$AIS_Type == "Tug"]
  metadata$npass_mmsis <-nships$n[nships$AIS_Type == "Passenger"]
  metadata$nsail_mmsis <-nships$n[nships$AIS_Type == "Sailing"]
  metadata$npleas_mmsis <-nships$n[nships$AIS_Type == "Pleasure"]
  metadata$nfish_mmsis <- nships$n[nships$AIS_Type == "Fishing"]
  metadata$ncargo_mmsis <- nships$n[nships$AIS_Type == "Cargo"]
  metadata$nother_mmsis <- nships$n[nships$AIS_Type == "Other"]
  metadata$ntotal_mmsis <- sum(nships$n)
  

  # Calculate total % position data removed from initial to final data set
  metadata$pctremoved <- round((length(unique(AIScsvDF5$MMSI)) - length(unique(AISjoined$MMSI)))/length(unique(AIScsvDF5$MMSI))*100, 2)
  
  # Thrown out for missing/incorrect lat/lon/MMSI, duplicate points, speed > 100 km/hr, outisde hex grid
  metadata$pctmissingwidth <- round(sum(is.na(AISjoined$DimWidth))/length(AISjoined$DimWidth)*100, 2)
  metadata$ pctmissinglength <- round(sum(is.na(AISjoined$DimLength))/length(AISjoined$DimLength)*100, 2)
  metadata$pctmissingSOG <-  round(sum(is.na(AISjoined$SOG))/length(AISjoined$DimLength)*100, 2)
  
  #########################################################################################
  # ADD IN DAY NIGHT HERE.... 
  
  
  # # Loop through each ship type and calculate summary statistics
  allTypes <- unique(AISjoined$AIS_Type)
  
  # Calculate summary stats for each ship type
  for (k in 1:length(allTypes)){
    # Select ship type
    AISfilteredType <- AISjoined %>%
      filter(AIS_Type==allTypes[k])
    
    # Calculate average speed within hex grid 
    joinOut <- AISfilteredType %>%
      group_by(hexID) %>%
      summarize(SOG_kts=mean(SOG, na.rm=T),
                SOGsd = sd(SOG, na.rm=T),
                EucSpeed_kts = mean(speed, na.rm=T)/1.852, # converted to knots 
                EucSpeedsd = sd(speed, na.rm=T)/1.852,
                ShipLength = mean(DimLength, na.rm=T), 
                nPts=n(), 
                nMMSI=length(unique(MMSI.x)),
                nOperDays=length(unique(AIS_ID)))
    
    colnames(joinOut)[2:ncol(joinOut)] <- paste0(colnames(joinOut)[2:ncol(joinOut)],"_",substring(allTypes[k],1,1))
    
    hexgrid <- left_join(hexgrid, joinOut, by="hexID")
  }
  
  # Calculate summary stats for ships > 65 feet length with at least one point over 10 kts 
  # Calculate average speed within hex grid 
  fastlong <- AISjoined %>%
    filter(SOG.x > 10 & DimLength > 19.812) # 19.812 m = 65 ft
  
  nfastlong_pts <- length(fastlong$AIS_Type)
  nfastlong_mmsis <- length(unique(fastlong$MMSI.x))
  nfastlong_aisids <- length(unique(fastlong$AIS_ID))
  
  fastlong <- fastlong %>% 
    group_by(hexID) %>%
    summarize(SOG_kts=mean(SOG.x, na.rm=T),
              SOGsd = sd(SOG.x, na.rm=T),
              Speed_kmh = mean(speed, na.rm=T),
              Speedsd = sd(speed, na.rm=T),
              ShipLength = mean(DimLength, na.rm=T), 
              nPts=n(), 
              nMMSI=length(unique(MMSI.x)),
              nOperDays=length(unique(AIS_ID)))
  
  colnames(fastlong)[2:ncol(fastlong)] <- paste0(colnames(fastlong)[2:ncol(fastlong)],"_LongFast")
  
  hexgrid <- left_join(hexgrid, fastlong, by="hexID")
  
  # Calculate summary stats for all ship types in aggregate
  # Calculate average speed within hex grid 
  allships <- AISjoined %>%
    group_by(hexID) %>%
    summarize(SOG_kts=mean(SOG.x, na.rm=T),
              SOGsd = sd(SOG.x, na.rm=T),
              Speed_kmh = mean(speed, na.rm=T),
              Speedsd = sd(speed, na.rm=T),
              ShipLength = mean(DimLength, na.rm=T), 
              nPts=n(), 
              nMMSI=length(unique(MMSI.x)),
              nOperDays=length(unique(AIS_ID)))
  
  colnames(allships)[2:ncol(allships)] <- paste0(colnames(allships)[2:ncol(allships)],"_All")
  
  hexgrid <- left_join(hexgrid, allships, by="hexID")
  
  hexpts <- st_as_sf(AISjoined, coords = c("long", "lat"), crs = 3338)
  
  # Save data in vector format
  # write_sf(hexgrid, paste0("../Data_Processed_TEST/Hex/SpeedHex_",MoName,"_",ndays,".shp"))
  write_sf(hexgrid, paste0("../Data_Processed/Hex/SpeedHex_",MoName,".shp"))
  # write_sf(AISjoined, paste0("../Data_Processed_TEST/Hex/SpeedPts_",MoName,"_",ndays,".shp"))
  # write_sf(hexpts, paste0("../Data_Processed_TEST/Hex/SpeedPts_",MoName,".shp"))
  
  
  # Save processing info to text file 
  hextime <- (proc.time() - start)[[3]]/60
  
  runtime <- proc.time() - starttime 
  runtime_min <- runtime[[3]]/60 
  summarystats <- data.frame(cbind(yr, mnth, runtime_min,all_pts, 
                                   posit_aisids, posit_mmsis, posit_pts,
                                   filt_aisids, filt_mmsis, filt_pts,
                                   inhex_aisids, inhex_mmsis, inhex_pts,
                                   speed_aisids, speed_mmsis, speed_pts,
                                   InSpeedNotLookup_mmsis, InSpeedNotLookup_npts, InLookupNotSpeed_mmsis,
                                   ntank_pts, ntank_aisids, ntank_mmsis,
                                   nfish_pts, nfish_aisids, nfish_mmsis,
                                   ncargo_pts, ncargo_aisids, ncargo_mmsis,
                                   nother_pts, nother_aisids, nother_mmsis,
                                   nfastlong_pts, nfastlong_aisids, nfastlong_mmsis,
                                   ntotal_pts,ntotal_mmsis, ntotal_aisids,
                                   pctremoved, pctmissingwidth, pctmissinglength, pctmissingSOG))
  write.csv(summarystats, paste0("../Data_Processed/Hex/Metadata_SpeedHex_",MoName,".csv"))
  
  
  runtimes <- data.frame(cbind(yr, mnth, runtime_min, importtime, dftime, distincttime, nothextime, speedtime, jointime, hextime)) 
  write.csv(runtimes, paste0("../Data_Processed/Hex/Runtimes_SpeedHex_",MoName,".csv"))
  # write.csv(runtimes, paste0("../Data_Processed_TEST/Hex/Runtimes_SpeedHex_",MoName,"_",ndays,".csv"))
  # print(runtimes)
  # return(runtimes)
}

####################################################################
####################### RUNNING SPEED SCRIPT ####################### 
####################################################################


# # Import hex grid
hexgrid <- st_read("../Data_Raw/BlankHexes.shp")

daynight <- TRUE

# Pull up list of AIS files
files <-  list.files("D:/AlaskaConservation_AIS_20210225/Data_Raw/2015/", pattern='.csv', full.names=T)

# Separate file names into monthly lists
jan <- files[grepl("-01-", files)]

csvList <- jan[1]

# Run the speed hex creation script
starthex1 <- proc.time()
FWS.AIS.SpeedHex(csvList, hexgrid)
endhex1 <- proc.time() - starthex1
# 
# 
# browseURL("https://www.youtube.com/watch?v=K1b8AhIsSYQ&ab_channel=RHINO")
# 
# 
# temp <- st_read("../Data_Processed_TEST/Hex/SpeedHex_2020-01.shp")

####################################################################
####################### PARALLELIZATION CODE ####################### 
####################################################################

# Pull up list of AIS files
files <- paste0("../Data_Raw/2015/", list.files("../Data_Raw/2015", pattern='.csv'))

# Separate file names into monthly lists
jan <- files[grepl("-01-", files)]
feb <- files[grepl("-02-", files)]
mar <- files[grepl("-03-", files)]
apr <- files[grepl("-04-", files)]
may <- files[grepl("-05-", files)]
jun <- files[grepl("-06-", files)]
jul <- files[grepl("-07-", files)]
aug <- files[grepl("-08-", files)]
sep <- files[grepl("-09-", files)]
oct <- files[grepl("-10-", files)]
nov <- files[grepl("-11-", files)]
dec <- files[grepl("-12-", files)]

# Create a list of lists of all csv file names grouped by month
csvsByMonth <- list(jan, feb, mar, apr, may, jun, jul, aug, sep, oct, nov, dec)

# Pull up hex grid
hexgrid <- st_read("../Data_Raw/RevisedHexes.shp")

## MSU HPCC: https://wiki.hpcc.msu.edu/display/ITH/R+workshop+tutorial#Rworkshoptutorial-Submittingparalleljobstotheclusterusing{doParallel}:singlenode,multiplecores
# Request a single node (this uses the "multicore" functionality)
registerDoParallel(cores=as.numeric(Sys.getenv("SLURM_CPUS_ON_NODE")[1]))

# create a blank list to store the results (I truncated the code before the ship-type coding, and just returned the sf of all that day's tracks so I didn't 
#       have to debug the raster part. If we're writing all results within the function - as written here and as I think we should do - the format of the blank list won't really matter.)
res=list()

# foreach and %dopar% work together to implement the parallelization
# note that you have to tell each core what packages you need (another reason to minimize library use), so it can pull those over
# I'm using tidyverse since it combines dplyr and tidyr into one library (I think)
res=foreach(i=1:12,.packages=c("maptools", "rgdal", "dplyr", "tidyr", "tibble", "sf", "foreach", "doParallel"),
            .errorhandling='pass',.verbose=T,.multicombine=TRUE) %dopar% FWS.AIS.SpeedHex(csvList=csvsByMonth[[i]], hexgrid=hexgrid)
# lapply(csvsByMonth, FWS.AIS)

# Elapsed time and running information
tottime <- proc.time() - start
tottime_min <- tottime[[3]]/60

cat("Time elapsed:", tottime_min, "\n")
cat("Currently registered backend:", getDoParName(), "\n")
cat("Number of workers used:", getDoParWorkers(), "\n")


########################################################################## 
############################# Post-Processing ############################
####################### Simplification of Hex Grid ####################### 
########################################################################## 

# Read in all hex files
filenames <-paste0("../Data_Processed_Hex/", 
                   list.files("../Data_Processed_Hex/", pattern='.shp'))

# Identify year and month for each file
yr <- substr(filenames, 32, 35)
mo <- substr(filenames, 37, 38)

# Read in shape fiels 
temphpcc <- lapply(filenames, st_read)

# Add year and month as attributes 
for(i in 1:length(temphpcc)){
  print(i)
  t <- temphpcc[[i]]
  t <- t %>% mutate(year = yr[i], month = mo[i])
  temphpcc[[i]] <- t
}

# Select columns of interest and rename to standard names
newshp <- lapply(temphpcc, function(x){x %>% 
    select(hexID, year, month,
           SOG_k_C, SOGsd_C, nMMSI_C, nOprD_C,
           SOG_k_T, SOGsd_T, nMMSI_T, nOprD_T,
           SOG_k_O, SOGsd_O, nMMSI_O, nOprD_O,
           SOG_k_F, SOGsd_F, nMMSI_F, nOprD_F,
           SOG__LF, SOGs_LF, nMMSI_L, nOpD_LF,
           SOG_k_A, SOGsd_A, nMMSI_A, nOprD_A) %>% 
    rename(SOG_C = SOG_k_C,
           SOG_T = SOG_k_T,
           SOG_O = SOG_k_O, 
           SOG_F = SOG_k_F, 
           SOG_L = SOG__LF,
           SOG_A = SOG_k_A, 
           SOGsd_L = SOGs_LF, 
           nOPrD_L = nOpD_LF)})
# Save new files 
for(i in 1:length(newshp)){
  t <- newshp[[i]]
  st_write(t, paste0("../Data_Processed_Hex/Simplified/SpeedHex_",yr[i],"-",mo[i],".shp"))
}
