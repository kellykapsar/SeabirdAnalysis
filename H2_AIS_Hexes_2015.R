################################################################################
# TITLE: Speed Hex Data Processing
# PURPOSE: This script takes raw AIS data from exactEarth and processes it into 
  # summary statistics aggregated into hexagonal polygons (generated by 
  # H1_HexGridCreation.R script).Summary statistics include number of ships,
  # number of operating days, average speed, and sd of speed on a monthly scale for 
  # cargo, fishing, tanker, other, long-fast, and all vessels combined.
# AUTHOR: Ben Sullender & Kelly Kapsar
# CREATED: 2021
# LAST UPDATED ON: 2022-06-09
# 
# NOTE: CODE DESIGNED TO RUN ON HPCC. 
################################################################################

# Start timer
start <- proc.time()

# Load libraries 
library(maptools)
library(rgdal)
library(dplyr)
library(tidyr)
library(purrr)
library(lubridate)
library(tibble)
library(sf)
library(foreach)
library(doParallel)

####################################################################
##################### AIS PROCESSING FUNCTION ######################
####################################################################

# INPUTS: A list of lists containing all daily csv file names for one year of AIS data.  
## Inner list = file paths/names for daily AIS csvs
## Outer list = a list of months 

# OUTPUTS: 
## Monthly vector files (hex grid) containing ship traffic summary statistics 
## by ship type and ship length. 

FWS.AIS.SpeedHex <- function(csvList, hexgrid, nightonly=TRUE){
  # start timer 
  starttime <- proc.time()
  
  ##################### Initial data import ######################
  # Start segment timer 
  start <- proc.time()
  
  # this will come in handy later. chars 28 to 34 = "yyyy-mm"
  # MoName <- substr(csvList[[1]][1],77, 83) # MY COMPUTER
  MoName <- substr(csvList[[1]][1],45, 51) # HPCC
  yr <- substr(MoName, 1, 4) 
  mnth <- substr(MoName, 6, 7)
  print(paste0("Processing",yr, mnth))
  
  # set up dfs for metadata and runtimes
  metadata <- data.frame(yr = yr, mnth=mnth)
  runtimes <- data.frame(yr = yr, mnth=mnth)
  
  # read in csv (specifying only columns that we want based on position in dataframe)
  temp <-  lapply(csvList, read.csv, header=TRUE, na.strings=c("","NA"),
                  colClasses = c(rep("character", 2), "NULL", "character", "NULL", "NULL", 
                                 "character", rep("NULL", 6), "character", "NULL", rep("character",8),
                                 "NULL", "character", "NULL", "character", "NULL", rep("character", 2), rep("NULL", 109)
                  ))
  
  
  AIScsv <- do.call(rbind , temp)
  
  metadata$orig_MMSIs <- length(unique(AIScsv$MMSI))
  metadata$orig_pts <- length(AIScsv$MMSI)
  
  runtimes$importtime <- (proc.time() - start)[[3]]/60
  
  # print(paste("Imported: ",yr, mnth))
  ##################### Data cleaning ######################
  start <- proc.time()
  
  # Convert character columns to numeric as needed
  numcols <- c(1:2, 6:12, 14:17)
  AIScsv[,numcols] <- lapply(AIScsv[,numcols], as.numeric)
  
  # print(paste("Numeric cols: ",yr, mnth))
  # Create df using only position messages (excluding type 27 which has increased location error)
  # For more info on message types see: https://www.marinfo.gc.ca/e-nav/docs/list-of-ais-messages-en.php
  AIScsvDF5 <- AIScsv %>%
    dplyr::select(MMSI,Latitude,Longitude,Time,Message_ID,SOG) %>%
    subset(!(Message_ID %in% c(5,24, 27)))
  
  metadata$messid_mmsis <- length(unique(AIScsvDF5$MMSI))
  metadata$messid_pts <- length(AIScsvDF5$MMSI)
  
  # print(paste("Position messages: ",yr, mnth))
  # Remove invalid lat/long values
  AIScsvDF4 <- AIScsvDF5 %>%
    filter(!is.na(Latitude)) %>%
    filter(!is.na(Longitude)) 
  
  metadata$invallatlon__mmsis <- length(unique(AIScsvDF4$MMSI))
  metadata$invallatlon__pts <- length(AIScsvDF4$MMSI)
  
  # print(paste("Inval lat/lon: ",yr, mnth))
  # Remove invalid MMSIs
  AIScsvDF3 <- AIScsvDF4 %>%
    dplyr::filter(nchar(trunc(abs(MMSI))) == 9)
  
  metadata$invalmmsi__mmsis <- length(unique(AIScsvDF3$MMSI))
  metadata$invalmmsi__pts <- length(AIScsvDF3$MMSI)
  
  # print(paste("Inval MMSI: ",yr, mnth))
  # Remove stationary aids to navigation
  AIScsvDF2 <- AIScsvDF3 %>%
    dplyr::filter(MMSI < 990000000) 
  
  metadata$aton_mmsis <- length(unique(AIScsvDF2$MMSI))
  metadata$aton_pts <- length(AIScsvDF2$MMSI)
  
  # Create AIS_ID field
  AIScsvDF <- AIScsvDF2 %>% add_column(AIS_ID = paste0(AIScsvDF2$MMSI,"-",substr(AIScsvDF2$Time,1,8)))
  
  # Identify and remove frost flowers 
  # (i.e. multiple messages from the same exact location sporadically transmitted throughout the day)
  ff <- AIScsvDF %>% 
    group_by(AIS_ID, Longitude, Latitude) %>% 
    summarize(n=n()) %>% filter(n > 2) %>% 
    mutate(tempid = paste0(AIS_ID, Longitude, Latitude))
  
  AISspeed4 <- AIScsvDF %>% 
    mutate(tempid = paste0(AIS_ID, Longitude, Latitude)) %>% 
    filter(!(tempid %in% ff$tempid))
  
  runtimes$dftime <- (proc.time() - start)[[3]]/60
  
  print(paste("Finished Cleaning ",yr, mnth))
  
  ##################### Speed filtering ######################
  
  # Convert to spatial data, get coords in projected format, and intersect with hex grid 
  AISspeed3 <- AISspeed4 %>%
    st_as_sf(coords=c("Longitude","Latitude"),crs=4326, remove=FALSE) %>%
    # project into Alaska Albers (or other CRS that doesn't create huge gap in mid-Bering with -180W and 180E)
    st_transform(crs=3338) %>%
    st_join(hexgrid["hexID"]) %>% 
    mutate(Time = as.POSIXct(Time, format="%Y%m%d_%H%M%OS", tz="GMT")) %>% # GMT == UTC
    # mutate(Time = as.POSIXct(Time, format="%Y%m%d_%H%M%OS")) %>% # GMT == UTC
    arrange(Time) 
  
  runtimes$dftime <- (proc.time() - start)[[3]]/60
  start <- proc.time()
  
  # Calculate the euclidean speed between points
  # Also remove successive duplicate points (i.e., same time stamp and/or same location)
  AISspeed3[, c("x", "y")] <- st_coordinates(AISspeed3)
  AISspeed2 <- AISspeed3 %>% 
    group_by(AIS_ID) %>%
    arrange(AIS_ID, Time) %>% 
    mutate(timediff = as.numeric(difftime(Time,lag(Time),units=c("hours"))),
           distdiff = sqrt((y-lag(y))^2 + (x-lag(x))^2)/1000) %>% 
    filter(timediff > 0) %>% 
    filter(distdiff > 0) %>% 
    mutate(speed = distdiff/timediff) 
  
  metadata$redund_aisids <- length(unique(AISspeed2$AIS_ID))
  metadata$redund_mmsi <- length(unique(AISspeed2$MMSI))
  metadata$redund_pts <- length(AISspeed2$MMSI)
  
  # Implement speed filter of 100 km/hr (also remove NA speed)
  AISspeed1 <- AISspeed2 %>% dplyr::filter(speed < 100) %>% dplyr::filter(!is.na(speed))
  
  metadata$speed_aisids <- length(unique(AISspeed1$AIS_ID))
  metadata$speed_mmsi <- length(unique(AISspeed1$MMSI))
  metadata$speed_pts <- length(AISspeed1$MMSI)
  
  # Remove points outside of hex grid 
  # system.time(joinTest <- joinTest_step1[!is.na(joinTest_step1$hexID),]) # Slow
  # system.time(na.omit(joinTest_step1[,c("hexID")])) # Slowest
  # system.time(joinTest_step1[-which(is.na(joinTest_step1$hexID)),]) # Slower 
  AISspeed0 <- subset(AISspeed1, !is.na(AISspeed1$hexID))  # Fastest
 
  AISspeed <- AISspeed0 %>% rename(long=Longitude, lat=Latitude)
  
  metadata$sinhex_aisids <- length(unique(AISspeed$AIS_ID))
  metadata$sinhex_mmsis <- length(unique(AISspeed$MMSI))
  metadata$sinhex_pts <- length(AISspeed$MMSI)
  
  runtimes$nothextime <- (proc.time() - start)[[3]]/60
  print(paste("Finished in hex and speed ",yr, mnth))
  

  
  # Calculate whether points are occurring during daytime or at night
  if(nightonly==TRUE){
    print(paste("Spatial ",yr, mnth))
    temp <- st_coordinates(st_transform(AISspeed, 4326))
    
    solarpos <- maptools::solarpos(temp, dateTime=AISspeed$Time, POSIXct.out=TRUE)
    sunrise <- maptools::sunriset(temp, dateTime=AISspeed$Time, direction="sunrise", POSIXct.out=TRUE)
    sunset <- maptools::sunriset(temp, dateTime=AISspeed$Time, direction="sunset", POSIXct.out=TRUE)
    
    print(paste("Sunrise",yr, mnth))
    AISspeed$solarpos <- solarpos[,2]
    AISspeed$timeofday <- as.factor(ifelse(AISspeed$solarpos > 0, "day","night"))
    
    AISspeed$sunrise <- sunrise$time
    AISspeed$sunset <- sunset$time

  }

  start <- proc.time()
  
  # create lookup table from static messages
  AISlookup1 <- AIScsv %>%
    add_column(DimLength = AIScsv$Dimension_to_Bow+AIScsv$Dimension_to_stern, 
               DimWidth = AIScsv$Dimension_to_port+AIScsv$Dimension_to_starboard) %>%
    dplyr::select(-Navigational_status, 
                  -SOG, 
                  -Longitude, 
                  -Latitude) %>%
    filter(Message_ID %in% c(5,24)) %>%
    filter(nchar(trunc(abs(MMSI))) == 9) 
  
  # Identify "best" static message from a given day 
  # Take all the static messages from a given month and down weight messages with a ship 
  # type of 0 or NA. Take the static messages and keeps the message with the highest weight
  # Code adapted from: https://stackoverflow.com/questions/72650475/take-unique-rows-in-r-but-keep-most-common-value-of-a-column-and-use-hierarchy
  wghts <- data.frame(poss = c(0, NA), nums = c(-10, -10))
  matched <- left_join(AISlookup1, wghts, by = c("Ship_Type"= "poss"))
  matched$nums[is.na(matched$nums)] <- 1
  data.table::setDT(matched)[, freq := .N, by = c("MMSI", "Ship_Type")]
  multiplied <- distinct(matched, MMSI, Ship_Type, .keep_all = TRUE)
  multiplied$mult <- multiplied$nums * multiplied$freq
  check <- multiplied[with(multiplied, order(MMSI, -mult)), ]
  AISlookup <- distinct(check, MMSI, .keep_all = TRUE) %>% dplyr::select(-nums, -freq, -mult)
  
  metadata$InSfNotLookup_mmsis <- length(AISspeed$MMSI[!(AISspeed$MMSI %in% AISlookup$MMSI)])
  metadata$InLookupNotSf_mmsis <- length(AISlookup$MMSI[!(AISlookup$MMSI %in% AISspeed$MMSI)])
  
  
  # Calculate number of ships with 0 values in dimensions and convert to NA
  metadata$nolength <- length(which(is.na(AISlookup$Dimension_to_Bow | AISlookup$Dimension_to_stern)))
  zerolength <- which(AISlookup$Dimension_to_Bow == 0 | AISlookup$Dimension_to_stern == 0)
  metadata$pctzerolength <- round((metadata$nolength + length(zerolength))/length(AISlookup$Dimension_to_Bow)*100,2)
  
  metadata$nowidth <- length(which(is.na(AISlookup$Dimension_to_port | AISlookup$Dimension_to_starboard)))
  zerowidth <- which(AISlookup$Dimension_to_port == 0 | AISlookup$Dimension_to_starboard == 0)
  metadata$pctzerowidth <- round((metadata$nowidth + length(zerowidth))/length(AISlookup$Dimension_to_port)*100,2)
  
  # Remove zero value rows for consideration of respective measurement
  # (i.e. if either bow or stern is zero, then both get NA 
  # and if either port or starboard is zero then both get NA)
  AISlookup$Dimension_to_Bow[zerolength] <- NA
  AISlookup$Dimension_to_stern[zerolength] <- NA
  AISlookup$Dimension_to_port[zerowidth] <- NA
  AISlookup$Dimension_to_starboard[zerowidth] <- NA
  

  # Join lookup table to the lines based on scramble mmsi
  AISjoined <- st_drop_geometry(AISspeed) %>%
    left_join(AISlookup,by="MMSI")
  
  # ID ship type 
  # link to ship type/numbers table: 
  # https://help.marinetraffic.com/hc/en-us/articles/205579997-What-is-the-significance-of-the-AIS-Shiptype-number-
  AISjoined$AIS_Type <- ifelse(substr(AISjoined$Ship_Type,1,1)==7, "Cargo",
                        ifelse(substr(AISjoined$Ship_Type,1,1)==8, "Tanker", 
                        ifelse(substr(AISjoined$Ship_Type,1,2)==30, "Fishing", 
                        ifelse(substr(AISjoined$Ship_Type,1,2)==52, "Tug",
                        ifelse(substr(AISjoined$Ship_Type,1,1)==6, "Passenger",
                        ifelse(substr(AISjoined$Ship_Type,1,2)==36, "Sailing",
                        ifelse(substr(AISjoined$Ship_Type,1,2)==37, "Pleasure", "Other")))))))
  AISjoined$AIS_Type[which(is.na(AISjoined$AIS_Type))] <- "Other"
  
  jointime <- (proc.time() - start)[[3]]/60
  start <- proc.time()
  print(paste("Finished join ",yr, mnth))
  
  # Number of points by ship type
  nships <- AISjoined %>% group_by(AIS_Type) %>% summarize(n=length(unique(MMSI)))
  metadata$ntank_mmsis <- nships$n[nships$AIS_Type == "Tanker"]
  metadata$ntug_mmsis <- nships$n[nships$AIS_Type == "Tug"]
  metadata$npass_mmsis <-nships$n[nships$AIS_Type == "Passenger"]
  metadata$nsail_mmsis <-nships$n[nships$AIS_Type == "Sailing"]
  metadata$npleas_mmsis <-nships$n[nships$AIS_Type == "Pleasure"]
  metadata$nfish_mmsis <- nships$n[nships$AIS_Type == "Fishing"]
  metadata$ncargo_mmsis <- nships$n[nships$AIS_Type == "Cargo"]
  metadata$nother_mmsis <- nships$n[nships$AIS_Type == "Other"]
  metadata$ntotal_mmsis <- sum(nships$n)
  

  # Calculate total % position data removed from initial to final data set
  metadata$pctremoved <- round((length(unique(AIScsvDF5$MMSI)) - length(unique(AISjoined$MMSI)))/length(unique(AIScsvDF5$MMSI))*100, 2)
  
  # Thrown out for missing/incorrect lat/lon/MMSI, duplicate points, speed > 100 km/hr, outisde hex grid
  metadata$pctmissingwidth <- round(sum(is.na(AISjoined$DimWidth))/length(AISjoined$DimWidth)*100, 2)
  metadata$ pctmissinglength <- round(sum(is.na(AISjoined$DimLength))/length(AISjoined$DimLength)*100, 2)
  metadata$pctmissingSOG <-  round(sum(is.na(AISjoined$SOG))/length(AISjoined$DimLength)*100, 2)
  
  # # Loop through each ship type and calculate summary statistics
  allTypes <- unique(AISjoined$AIS_Type)
  
  # Calculate summary stats for each ship type
  for (k in 1:length(allTypes)){
    # Select ship type
    AISfilteredType <- AISjoined %>%
      filter(AIS_Type==allTypes[k])
    
    if(nightonly == TRUE){
      
      # Calculate average speed within hex grid 
      joinOut <- AISfilteredType %>%
        group_by(hexID) %>%
        summarize(Shp=length(unique(MMSI)),
                  OpD=length(unique(AIS_ID)))
      joinOutDay <- AISfilteredType %>%
        filter(timeofday != "night") %>% 
        group_by(hexID) %>%
        summarize(D_Shp=length(unique(MMSI)),
                  D_OpD=length(unique(AIS_ID)))
      joinOutNight <- AISfilteredType %>%
        filter(timeofday == "night") %>% 
        group_by(hexID) %>%
        summarize(N_Shp=length(unique(MMSI)),
                  N_OpD=length(unique(AIS_ID)))
      joinOutNew <- left_join(joinOut, joinOutNight, by=c("hexID"))
      joinOutNew <- left_join(joinOutNew, joinOutDay, by=c("hexID"))
      joinOutNew <- joinOutNew %>% 
        mutate_at(c(1:ncol(joinOutNew)), ~replace_na(.,0))
    }
    if(nightonly == FALSE){
      
      # Calculate average speed within hex grid 
      joinOutNew <- AISfilteredType %>%
        group_by(hexID) %>%
        summarize(SOG=mean(SOG, na.rm=T),
                  Sd = sd(SOG, na.rm=T),
                  Len = mean(DimLength, na.rm=T), 
                  nPt=n(), 
                  nShp =length(unique(MMSI)),
                  OpD=length(unique(AIS_ID)))
    }
    
    colnames(joinOutNew)[2:ncol(joinOutNew)] <- paste0(colnames(joinOutNew)[2:ncol(joinOutNew)],"_",substring(allTypes[k],1,2))
    
    hexgrid <- left_join(hexgrid, joinOutNew, by="hexID")
  }
  
  # Calculate summary stats for all ship types in aggregate
  # Calculate average speed within hex grid 
  allShips <- AISjoined %>%
    group_by(hexID) %>%
    summarize(nShp=length(unique(MMSI)),
              OpD=length(unique(AIS_ID)))
  allShipsDay <- AISjoined %>%
    filter(timeofday != "night") %>% 
    group_by(hexID) %>%
    summarize(D_nShp =length(unique(MMSI)),
              D_OpD=length(unique(AIS_ID)))
    allShipsNight <- AISjoined %>%
    filter(timeofday == "night") %>% 
    group_by(hexID) %>%
    summarize(N_nShp =length(unique(MMSI)),
              N_OpD=length(unique(AIS_ID)))
  allShipsNew <- left_join(allShips, allShipsNight, by=c("hexID"))
  allShipsNew <- left_join(allShipsNew, allShipsDay, by=c("hexID"))
  
  colnames(allShipsNew)[2:ncol(allShipsNew)] <- paste0(colnames(allShipsNew)[2:ncol(allShipsNew)],"_Al")
  
  hexgrid <- left_join(hexgrid, allShipsNew, by="hexID")
  
  hexgrid$year <- yr
  hexgrid$month <- mnth
  
  
  
  print(paste("Finished hex calcs ",yr, mnth))
  # hexpts <- st_as_sf(AISjoined, coords = c("x", "y"), crs = 3338)
  
  # Save data in vector format
  # write_sf(hexgrid, paste0("../Data_Processed_TEST/Hex/SpeedHex_",MoName,"_",ndays,".shp"))
  write_sf(hexgrid, paste0("../Data_Processed/Hex/Hex_",MoName,"_DayNight",nightonly,".shp"))
  # write_sf(AISjoined, paste0("../Data_Processed_TEST/Hex/SpeedPts_",MoName,"_",ndays,".shp"))
  # write_sf(hexpts, paste0("../Data_Processed_TEST/Hex/SpeedPts_",MoName,".shp"))
  
  
  # Save processing info to text file 
  runtimes$hextime <- (proc.time() - start)[[3]]/60
  
  runtimes$runtime <- (proc.time() - starttime)[[3]]
  runtimes$runtime_min <- runtimes$runtime/60 

  write.csv(metadata, paste0("../Data_Processed/Hex/HexMetadata_",MoName,"_DayNight",nightonly,".csv"))
  
  
  write.csv(runtimes, paste0("../Data_Processed/Hex/HexRuntimes_",MoName,"_DayNight",nightonly,".csv"))
  # write.csv(runtimes, paste0("../Data_Processed_TEST/Hex/Runtimes_SpeedHex_",MoName,"_",ndays,".csv"))
  # print(runtimes)
  # return(runtimes)
}

####################################################################
####################### RUNNING SPEED SCRIPT ####################### 
####################################################################


# # Import hex grid
# hexgrid <- st_read("../Data_Raw/BlankHexes.shp")
# 
# nightonly <- TRUE
# 
# # Pull up list of AIS files
# files <-  list.files("D:/AlaskaConservation_AIS_20210225/Data_Raw/2015/", pattern='.csv', full.names=T)
# 
# # Separate file names into monthly lists
# jan <- files[grepl("-01-", files)]
# 
# csvList <- jan[27:28]

# # Separate file names into monthly lists
# jun <- files[grepl("-06-", files)]
#
# csvList <- jun[27:28]
# 
# # Run the speed hex creation script
# starthex1 <- proc.time()
# FWS.AIS.SpeedHex(csvList, hexgrid)
# endhex1 <- proc.time() - starthex1
# 
# 
# browseURL("https://www.youtube.com/watch?v=K1b8AhIsSYQ&ab_channel=RHINO")
# 
# 
############# MANUAL CHECK OF SUNRISE/SUNSET AND POSITION IN LOCAL TIME ############
# AISspeedTest <- AISspeed %>% dplyr::select(MMSI, Time, solarpos, todsolar, sunrise, sunset, todriseset) %>%
#   mutate(long = st_coordinates(st_transform(., 4326))[,1], 
#          lat = st_coordinates(st_transform(., 4326))[,2], 
#          timezone = lutz::tz_lookup_coords(lat, long, method="accurate"))
# 
# test <- AISspeed %>% dplyr::select(MMSI, Time, solarpos, todsolar, sunrise, sunset, todriseset) %>%
#                      mutate(long = st_coordinates(st_transform(., 4326))[,1], 
#                             lat = st_coordinates(st_transform(., 4326))[,2], 
#                             timezone = lutz::tz_lookup_coords(lat = lat, lon = long, method = "accurate"),
#                             TimeLocal = purrr::map2(.x = Time, .y = timezone, 
#                                     .f = function(x, y) {lubridate::with_tz(time = x, tzone = y)})) %>% 
#                             unnest(TimeLocal) %>% 
#                      mutate(sunsetLocal = purrr::map2(.x = sunset, .y = timezone, 
#                                      .f = function(x, y) {lubridate::with_tz(time = x, tzone = y)})) %>% 
#                             unnest(sunsetLocal) %>% 
#                      mutate(sunriseLocal = purrr::map2(.x = sunrise, .y = timezone, 
#                                  .f = function(x, y) {lubridate::with_tz(time = x, tzone = y)})) %>% 
#                             unnest(sunriseLocal)


# 61089 discrepancies between solar position and sunrise/set calcualtions of day 
# 61089/502438 = ~12% When I manually scroll through them, it looks like the solar pos is a better fit 

# length(test$todsolar[which(test$todsolar == "day")])/length(test$todsolar)
# 38% of points during daytime from Jan 27-28
# 69% of points during daytime from Jun 27-28

# t <- test %>% 
#   mutate(latbin = round(lat, 0), 
#                      longbin = round(long, 0)) %>% 
#   group_by(latbin) %>% 
#   summarize(pctday = sum(todsolar == "day")/length(todsolar), 
#             npts = n())
# 
# plot(t$latbin, t$pctday)
# plot(t$longbin, t$pctday)

####################################################################
####################### PARALLELIZATION CODE ####################### 
####################################################################

# Pull up list of AIS files
files <- paste0("../Data_Raw/2015/", list.files("../Data_Raw/2015", pattern='.csv'))

# Separate file names into monthly lists
jan <- files[grepl("-01-", files)]
feb <- files[grepl("-02-", files)]
mar <- files[grepl("-03-", files)]
apr <- files[grepl("-04-", files)]
may <- files[grepl("-05-", files)]
jun <- files[grepl("-06-", files)]
jul <- files[grepl("-07-", files)]
aug <- files[grepl("-08-", files)]
sep <- files[grepl("-09-", files)]
oct <- files[grepl("-10-", files)]
nov <- files[grepl("-11-", files)]
dec <- files[grepl("-12-", files)]

# Create a list of lists of all csv file names grouped by month
csvsByMonth <- list(jan, feb, mar, apr, may, jun, jul, aug, sep, oct, nov, dec)

# Pull up hex grid
hexgrid <- st_read("../Data_Raw/BlankHexes.shp")

## MSU HPCC: https://wiki.hpcc.msu.edu/display/ITH/R+workshop+tutorial#Rworkshoptutorial-Submittingparalleljobstotheclusterusing{doParallel}:singlenode,multiplecores
# Request a single node (this uses the "multicore" functionality)
registerDoParallel(cores=as.numeric(Sys.getenv("SLURM_CPUS_ON_NODE")[1]))

# create a blank list to store the results (I truncated the code before the ship-type coding, and just returned the sf of all that day's tracks so I didn't 
#       have to debug the raster part. If we're writing all results within the function - as written here and as I think we should do - the format of the blank list won't really matter.)
res=list()

# foreach and %dopar% work together to implement the parallelization
# note that you have to tell each core what packages you need (another reason to minimize library use), so it can pull those over
# I'm using tidyverse since it combines dplyr and tidyr into one library (I think)
res=foreach(i=1:12,.packages=c("maptools", "rgdal", "dplyr", "tidyr", "tibble", "sf", "foreach", "doParallel"),
            .errorhandling='pass',.verbose=T,.multicombine=TRUE) %dopar% FWS.AIS.SpeedHex(csvList=csvsByMonth[[i]], hexgrid=hexgrid, nightonly=TRUE)
# lapply(csvsByMonth, FWS.AIS)

# Elapsed time and running information
tottime <- proc.time() - start
tottime_min <- tottime[[3]]/60

cat("Time elapsed:", tottime_min, "\n")
cat("Currently registered backend:", getDoParName(), "\n")
cat("Number of workers used:", getDoParWorkers(), "\n")



